[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Bachelor of Science in Statistics\nFudan University, Shanghai, 2016-2021\nMaster of Science in Biostatistics\nJohns Hopkins Bloomberg School of Public Health, 2021-2023\nDoctor of Philosophy in Biostatistics, Ongoing\nJohns Hopkins Bloomberg School of Public Health, 2023-2028"
  },
  {
    "objectID": "about.html#education-experience",
    "href": "about.html#education-experience",
    "title": "About Me",
    "section": "",
    "text": "Bachelor of Science in Statistics\nFudan University, Shanghai, 2016-2021\nMaster of Science in Biostatistics\nJohns Hopkins Bloomberg School of Public Health, 2021-2023\nDoctor of Philosophy in Biostatistics, Ongoing\nJohns Hopkins Bloomberg School of Public Health, 2023-2028"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\nMy research mostly focuses on microbiome studies. More specifically, I conduct methodological research aiming to develop more accurate and powerful statistical models for microbiome data analysis. With Dr. Ni Zhao, an associate professor in the department, I have worked on a few projects including “Differential Analysis through Proportional Hazards Model”, “Reliability and Effectiveness of Microbiome Rarefaction”, and “Longitudinal Analysis of Microbiome Data via Tensor Decomposition”."
  },
  {
    "objectID": "about.html#personal-life",
    "href": "about.html#personal-life",
    "title": "About Me",
    "section": "Personal Life",
    "text": "Personal Life\nI particularly enjoy music, especially musical theaters and classical music. I go to classical music concerts and musical shows almost weekly. My favorite composer is Antonio Vivaldi.\n\n\n\nSource: https://www.nationalgeographic.com/premium/article/antonio-vivaldi-composer-four-seasons"
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example Analysis",
    "section": "",
    "text": "source: https://www.myvmc.com/banners-heart-health-centre/ecg-ekg-electrocardiogram/"
  },
  {
    "objectID": "example_analysis.html#research-background-and-data-source",
    "href": "example_analysis.html#research-background-and-data-source",
    "title": "Example Analysis",
    "section": "Research Background and Data Source",
    "text": "Research Background and Data Source\nThis example analysis focuses on predicting the occurrence of heart disease using data from demographics, physical tests, and medical examinations. We will employ logistic regression with LASSO, evaluate their performances, and find significant predictors.\nThis analysis aims to help scientists as well as clinicians who try to understand what roles different factors play in predicting heart disease and provide a prediction tool in the real clinical setting.\nData used in this analysis comes from the UCI Machine Learning Repository. Data files and documentation can be accessed here. The whole dataset consists of 4 parts, but this analysis only uses the Cleveland database."
  },
  {
    "objectID": "example_analysis.html#data-processing",
    "href": "example_analysis.html#data-processing",
    "title": "Example Analysis",
    "section": "Data Processing",
    "text": "Data Processing\nBefore analysis, a few steps of data processing are needed. Column names are replaced by meaningful abbreviations and some variables are converted to the factor format according to the documentation.\n\n\n\n\n\n\nCaution\n\n\n\nTwo variables ca and thal contain missing values, and are removed from the dataset for the sake of simplicity. This might cause some trouble, and we may use imputation to fix this in future studies.\n\n\n\nlibrary(tidyverse)\ndata_raw = read.table(\"processed.cleveland.data\", sep = \",\")\n\ndata = data_raw %&gt;%\n    rename(age = V1, sex = V2, cp = V3, trestbps = V4, chol = V5,\n           fbs = V6, restecg = V7, thalach = V8, exang = V9, oldpeak = V10,\n           slope = V11, ca = V12, thal = V13, num = V14) %&gt;%\n    select(-ca, -thal) %&gt;%\n    mutate(sex = factor(sex),\n           cp = factor(cp),\n           fbs = factor(fbs),\n           restecg = factor(restecg),\n           exang = factor(exang),\n           slope = factor(slope),\n           hd = factor((num &gt; 0)*1)) %&gt;%\n    select(-num)\n\n\nset.seed(1630)\nn = nrow(data)\ndata$test = sample(rep(c(0, 1), times = c(round(0.75*n), n-round(0.75*n))))\n\ndata_train = filter(data, test == 0) %&gt;% select(-test)\ndata_test = filter(data, test == 1) %&gt;% select(-test)"
  },
  {
    "objectID": "example_analysis.html#descriptive-analysis",
    "href": "example_analysis.html#descriptive-analysis",
    "title": "Example Analysis",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis\nDensities and bars are plotted for numeric variables and factor variables, respectively.\n\n\n\n\n\n\nNote\n\n\n\nSome variables such as age, thalach, cp and exang demonstrate relations to heart disease status.\n\n\n\n\nShow code\ndata %&gt;%\n    select(where(is.numeric), hd) %&gt;%\n    pivot_longer(-hd, names_to = \"variables\", values_to = \"values\") %&gt;%\n    ggplot() +\n        geom_density(aes(x = values, color = hd), ) +\n        facet_wrap(variables~., scales = \"free\") +\n        labs(title = \"Density plots of numeric varibles\", subtitle = \"Stratified by heart disease status: 1-diseased\", caption = \"Data source: UCI Machine Learning Repository\")\n\n\n\n\n\n\n\nShow code\ndata %&gt;%\n    select(where(is.factor)) %&gt;%\n    pivot_longer(-hd, names_to = \"variables\", values_to = \"values\") %&gt;%\n    ggplot() +\n        geom_bar(aes(x = values, fill = hd), stat = \"count\", position = \"dodge\") +\n        facet_wrap(variables~., scales = \"free_x\") +\n        labs(title = \"Bar plots of factor varibles\", subtitle = \"Stratified by heart disease status: 1-diseased\",\n             caption = \"Data source: UCI Machine Learning Repository\")"
  },
  {
    "objectID": "example_analysis.html#data-analysis",
    "href": "example_analysis.html#data-analysis",
    "title": "Example Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\nLogistic regression including all predictors as well as lasso logistic regression are fitted. Cross-validation is used to find the best parameter \\(\\lambda\\) for Lasso.\n\nset.seed(1563)\n\nlogi = glm(hd~., data = data_train, family = \"binomial\")\npredict_logi = factor((predict(logi, data_test, type = \"response\") &gt; 0.5) * 1)\n\n\nlibrary(glmnet)\n\ny = data_train$hd == \"1\"\nX_train = model.matrix(hd~., data = data_train)[,-1]\nX_test = model.matrix(hd~., data = data_test)[,-1]\n\nlambda = cv.glmnet(X_train, y, alpha = 1)$lambda.min\nlogi_lasso = glmnet(X_train, y, family = \"binomial\", alpha = 1, lambda = lambda)\n\n\npredict_logi_lasso = factor((predict(logi_lasso, X_test, type = \"response\") &gt; 0.5) * 1)\n\n\nsummary(logi)\n\n\nCall:\nglm(formula = hd ~ ., family = \"binomial\", data = data_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2602  -0.6431  -0.1926   0.5167   2.3891  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -7.725202   3.208535  -2.408 0.016053 *  \nage          0.041665   0.025959   1.605 0.108484    \nsex1         1.936566   0.511034   3.790 0.000151 ***\ncp2          1.422578   0.817376   1.740 0.081785 .  \ncp3          0.489325   0.720488   0.679 0.497038    \ncp4          2.374920   0.729961   3.253 0.001140 ** \ntrestbps     0.005043   0.011665   0.432 0.665477    \nchol         0.008781   0.004710   1.864 0.062256 .  \nfbs1         0.189622   0.567755   0.334 0.738391    \nrestecg1     0.626915   1.600261   0.392 0.695237    \nrestecg2     0.641368   0.387736   1.654 0.098100 .  \nthalach     -0.014026   0.011470  -1.223 0.221382    \nexang1       0.559909   0.442171   1.266 0.205416    \noldpeak      0.705294   0.228179   3.091 0.001995 ** \nslope2       0.988993   0.459823   2.151 0.031491 *  \nslope3      -0.654558   1.012168  -0.647 0.517833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 313.70  on 226  degrees of freedom\nResidual deviance: 179.75  on 211  degrees of freedom\nAIC: 211.75\n\nNumber of Fisher Scoring iterations: 5\n\npredict(logi_lasso, X_test, type = \"coefficient\")\n\n16 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept) -3.666945730\nage          0.025940551\nsex1         1.194600748\ncp2          .          \ncp3         -0.101590294\ncp4          1.290922145\ntrestbps     .          \nchol         0.004610698\nfbs1         .          \nrestecg1     .          \nrestecg2     0.313835657\nthalach     -0.010740660\nexang1       0.434325068\noldpeak      0.437614473\nslope2       0.811598169\nslope3       .          \n\n\n\n\nShow code\naccuracy_logi = mean(predict_logi == data_test$hd)\naccuracy_logi_lasso = mean(predict_logi_lasso == data_test$hd)\n\nsensitivity_logi = mean(predict_logi[data_test$hd == \"1\"] == data_test$hd[data_test$hd == \"1\"])\nsensitivity_logi_lasso = mean(predict_logi_lasso[data_test$hd == \"1\"] == data_test$hd[data_test$hd == \"1\"])\n\nspecificity_logi = mean(predict_logi[data_test$hd == \"0\"] == data_test$hd[data_test$hd == \"0\"])\nspecificity_logi_lasso = mean(predict_logi_lasso[data_test$hd == \"0\"] == data_test$hd[data_test$hd == \"0\"])\n\nresult = matrix(c(accuracy_logi, sensitivity_logi, specificity_logi,\n                  accuracy_logi_lasso, sensitivity_logi_lasso, specificity_logi_lasso), c(3,2))\nrownames(result) = c(\"Accuracy\", \"Sensitivity\", \"Specificity\")\ncolnames(result) = c(\"Logistic regression\", \"LASSO logistic regression\")\n\nknitr::kable(\n  round(result, 4)\n)\n\n\n\n\n\n\n\nLogistic regression\nLASSO logistic regression\n\n\n\n\nAccuracy\n0.8026\n0.7763\n\n\nSensitivity\n0.7879\n0.7879\n\n\nSpecificity\n0.8140\n0.7674\n\n\n\n\n\n\nShow code\ndata.frame(logtistic = predict_logi,\n           logtistic_lasso = predict_logi_lasso,\n           test = data_test$hd) %&gt;%\n    ggplot() +\n        geom_jitter(aes(x = logtistic, y = logtistic_lasso, color = test), width = 0.2, height = 0.2) +\n        labs(x = \"Logistic regression\", y = \"LASSO logistic regression\", color = \"True status\",\n             title = \"Prediction results\", subtitle = \"Stratified by heart disease status: 1-diseased\",\n             caption = \"Data source: UCI Machine Learning Repository\")"
  },
  {
    "objectID": "example_analysis.html#conclusion",
    "href": "example_analysis.html#conclusion",
    "title": "Example Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nAs predictive models logistic regression and LASSO logistic regression perform fairly well. Logistic regression has slightly better performance in terms of accuracy (80.26% vs 77.63%) and specificity (81.40% vs 76.74%). The two models agree on all but two sample’s prediction results. As for important predictive factors, sex, cp(chest pain), oldpeak (ST depression induced by exercise relative to rest), and slope (slope of the peak exercise ST segment) show significant results. Male patients with asymptomatic chest pain and have a flat slope of the peak exercise ST segment are more likely to have heart disease. Scientists could look out for mechanisms under the surface and clinicians might want to give extra attention to such patients."
  },
  {
    "objectID": "example_analysis.html#functions-used",
    "href": "example_analysis.html#functions-used",
    "title": "Example Analysis",
    "section": "Functions Used",
    "text": "Functions Used\ntidyverse: rename, select, mutate, filter and pivot_longer.\nggplot: geom_density, geom_bar and geom_jitter."
  },
  {
    "objectID": "example_analysis.html#reference",
    "href": "example_analysis.html#reference",
    "title": "Example Analysis",
    "section": "Reference",
    "text": "Reference\n\nTibshirani (2013) Steinbrunn (1989) Heart Disease (1988)\n\nHeart Disease. 1988. UCI Machine Learning Repository. https://archive.ics.uci.edu/dataset/45/heart+disease.\n\n\nSteinbrunn, R. Detrano，A. Jánosi，W. 1989. “International Application of a New Probability Algorithm for the Diagnosis of Coronary Artery Disease.” American Journal of Cardiology.\n\n\nTibshirani, Gareth James，Daniela Witten，Trevor Hastie，Robert. 2013. An Introduction to Statistical Learning with Applications in r. Springer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project 1",
    "section": "",
    "text": "Greetings to whoever is looking at this! My name is Shengtao Wang. I am a first-year PhD student in the Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health. Welcome to my website!"
  }
]