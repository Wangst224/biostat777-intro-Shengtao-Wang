{
  "hash": "e09d07ee7d4d7b210b0c174d1cb42841",
  "result": {
    "markdown": "---\ntitle: \"Example Analysis\"\nbibliography: references.bib\nformat: html\n---\n\n![source: https://www.myvmc.com/banners-heart-health-centre/ecg-ekg-electrocardiogram/](ECG.jpg){fig-align=\"center\"} \n\n## Research Background and Data Source\nThis example analysis focuses on predicting the occurrence of heart disease using data from demographics, physical tests, and medical examinations. We will employ logistic regression with LASSO, evaluate their performances, and find significant predictors.\n\nThis analysis aims to help scientists as well as clinicians who try to understand what roles different factors play in predicting heart disease and provide a prediction tool in the real clinical setting.\n\nData used in this analysis comes from the UCI Machine Learning Repository. Data files and documentation can be accessed [here](https://archive.ics.uci.edu/dataset/45/heart+disease). The whole dataset consists of 4 parts, but this analysis only uses the Cleveland database.\n\n## Data Processing\nBefore analysis, a few steps of data processing are needed. Column names are replaced by meaningful abbreviations and some variables are converted to the factor format according to the documentation.\n\n::: {.callout-caution}\nTwo variables `ca` and `thal` contain missing values, and are removed from the dataset for the sake of simplicity. This might cause some trouble, and we may use imputation to fix this in future studies.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata_raw = read.table(\"processed.cleveland.data\", sep = \",\")\n\ndata = data_raw %>%\n    rename(age = V1, sex = V2, cp = V3, trestbps = V4, chol = V5,\n           fbs = V6, restecg = V7, thalach = V8, exang = V9, oldpeak = V10,\n           slope = V11, ca = V12, thal = V13, num = V14) %>%\n    select(-ca, -thal) %>%\n    mutate(sex = factor(sex),\n           cp = factor(cp),\n           fbs = factor(fbs),\n           restecg = factor(restecg),\n           exang = factor(exang),\n           slope = factor(slope),\n           hd = factor((num > 0)*1)) %>%\n    select(-num)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1630)\nn = nrow(data)\ndata$test = sample(rep(c(0, 1), times = c(round(0.75*n), n-round(0.75*n))))\n\ndata_train = filter(data, test == 0) %>% select(-test)\ndata_test = filter(data, test == 1) %>% select(-test)\n```\n:::\n\n\n## Descriptive Analysis\nDensities and bars are plotted for numeric variables and factor variables, respectively.\n\n::: {.callout-note}\nSome variables such as `age`, `thalach`, `cp` and `exang` demonstrate relations to heart disease status.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\ndata %>%\n    select(where(is.numeric), hd) %>%\n    pivot_longer(-hd, names_to = \"variables\", values_to = \"values\") %>%\n    ggplot() +\n        geom_density(aes(x = values, color = hd), ) +\n        facet_wrap(variables~., scales = \"free\") +\n        labs(title = \"Density plots of numeric varibles\", subtitle = \"Stratified by heart disease status: 1-diseased\", caption = \"Data source: UCI Machine Learning Repository\")\n```\n\n::: {.cell-output-display}\n![](example_analysis_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\ndata %>%\n    select(where(is.factor)) %>%\n    pivot_longer(-hd, names_to = \"variables\", values_to = \"values\") %>%\n    ggplot() +\n        geom_bar(aes(x = values, fill = hd), stat = \"count\", position = \"dodge\") +\n        facet_wrap(variables~., scales = \"free_x\") +\n        labs(title = \"Bar plots of factor varibles\", subtitle = \"Stratified by heart disease status: 1-diseased\",\n             caption = \"Data source: UCI Machine Learning Repository\")\n```\n\n::: {.cell-output-display}\n![](example_analysis_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Data Analysis\nLogistic regression including all predictors as well as lasso logistic regression are fitted. Cross-validation is used to find the best parameter $\\lambda$ for Lasso.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1563)\n\nlogi = glm(hd~., data = data_train, family = \"binomial\")\npredict_logi = factor((predict(logi, data_test, type = \"response\") > 0.5) * 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\ny = data_train$hd == \"1\"\nX_train = model.matrix(hd~., data = data_train)[,-1]\nX_test = model.matrix(hd~., data = data_test)[,-1]\n\nlambda = cv.glmnet(X_train, y, alpha = 1)$lambda.min\nlogi_lasso = glmnet(X_train, y, family = \"binomial\", alpha = 1, lambda = lambda)\n\n\npredict_logi_lasso = factor((predict(logi_lasso, X_test, type = \"response\") > 0.5) * 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(logi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hd ~ ., family = \"binomial\", data = data_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2602  -0.6431  -0.1926   0.5167   2.3891  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -7.725202   3.208535  -2.408 0.016053 *  \nage          0.041665   0.025959   1.605 0.108484    \nsex1         1.936566   0.511034   3.790 0.000151 ***\ncp2          1.422578   0.817376   1.740 0.081785 .  \ncp3          0.489325   0.720488   0.679 0.497038    \ncp4          2.374920   0.729961   3.253 0.001140 ** \ntrestbps     0.005043   0.011665   0.432 0.665477    \nchol         0.008781   0.004710   1.864 0.062256 .  \nfbs1         0.189622   0.567755   0.334 0.738391    \nrestecg1     0.626915   1.600261   0.392 0.695237    \nrestecg2     0.641368   0.387736   1.654 0.098100 .  \nthalach     -0.014026   0.011470  -1.223 0.221382    \nexang1       0.559909   0.442171   1.266 0.205416    \noldpeak      0.705294   0.228179   3.091 0.001995 ** \nslope2       0.988993   0.459823   2.151 0.031491 *  \nslope3      -0.654558   1.012168  -0.647 0.517833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 313.70  on 226  degrees of freedom\nResidual deviance: 179.75  on 211  degrees of freedom\nAIC: 211.75\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n\n```{.r .cell-code}\npredict(logi_lasso, X_test, type = \"coefficient\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept) -3.666945730\nage          0.025940551\nsex1         1.194600748\ncp2          .          \ncp3         -0.101590294\ncp4          1.290922145\ntrestbps     .          \nchol         0.004610698\nfbs1         .          \nrestecg1     .          \nrestecg2     0.313835657\nthalach     -0.010740660\nexang1       0.434325068\noldpeak      0.437614473\nslope2       0.811598169\nslope3       .          \n```\n:::\n:::\n\n::: {.cell .column-margin}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\naccuracy_logi = mean(predict_logi == data_test$hd)\naccuracy_logi_lasso = mean(predict_logi_lasso == data_test$hd)\n\nsensitivity_logi = mean(predict_logi[data_test$hd == \"1\"] == data_test$hd[data_test$hd == \"1\"])\nsensitivity_logi_lasso = mean(predict_logi_lasso[data_test$hd == \"1\"] == data_test$hd[data_test$hd == \"1\"])\n\nspecificity_logi = mean(predict_logi[data_test$hd == \"0\"] == data_test$hd[data_test$hd == \"0\"])\nspecificity_logi_lasso = mean(predict_logi_lasso[data_test$hd == \"0\"] == data_test$hd[data_test$hd == \"0\"])\n\nresult = matrix(c(accuracy_logi, sensitivity_logi, specificity_logi,\n                  accuracy_logi_lasso, sensitivity_logi_lasso, specificity_logi_lasso), c(3,2))\nrownames(result) = c(\"Accuracy\", \"Sensitivity\", \"Specificity\")\ncolnames(result) = c(\"Logistic regression\", \"LASSO logistic regression\")\n\nknitr::kable(\n  round(result, 4)\n)\n```\n\n::: {.cell-output-display}\n|            | Logistic regression| LASSO logistic regression|\n|:-----------|-------------------:|-------------------------:|\n|Accuracy    |              0.8026|                    0.7763|\n|Sensitivity |              0.7879|                    0.7879|\n|Specificity |              0.8140|                    0.7674|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code\"}\ndata.frame(logtistic = predict_logi,\n           logtistic_lasso = predict_logi_lasso,\n           test = data_test$hd) %>%\n    ggplot() +\n        geom_jitter(aes(x = logtistic, y = logtistic_lasso, color = test), width = 0.2, height = 0.2) +\n        labs(x = \"Logistic regression\", y = \"LASSO logistic regression\", color = \"True status\",\n             title = \"Prediction results\", subtitle = \"Stratified by heart disease status: 1-diseased\",\n             caption = \"Data source: UCI Machine Learning Repository\")\n```\n\n::: {.cell-output-display}\n![](example_analysis_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\nAs predictive models logistic regression and LASSO logistic regression perform fairly well. Logistic regression has slightly better performance in terms of accuracy (80.26% vs 77.63%) and specificity (81.40% vs 76.74%). The two models agree on all but two sample's prediction results. As for important predictive factors, `sex`, `cp`(chest pain), `oldpeak` (ST depression induced by exercise relative to rest), and `slope` (slope of the peak exercise ST segment) show significant results. Male patients with asymptomatic chest pain and have a flat slope of the peak exercise ST segment are more likely to have heart disease. Scientists could look out for mechanisms under the surface and clinicians might want to give extra attention to such patients.\n\n## Functions Used\n`tidyverse`: `rename`, `select`, `mutate`, `filter` and `pivot_longer`.\n\n`ggplot`: `geom_density`, `geom_bar` and `geom_jitter`.\n\n## Reference\n\n::: {#refs}\n@ISLR\n@Detrano1989\n@UCIhd\n:::",
    "supporting": [
      "example_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}